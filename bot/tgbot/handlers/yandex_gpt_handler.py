import json
import os
import re
from html import unescape

from aiogram import types
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from openai import AsyncOpenAI

from bot.tgbot.databases.pay_db import getBannedUserId, getUserPay
from config import BASE_DIR, MAX_BOT_MSG_LENGTH, VECTOR_DB_PATH, load_config, logger_bot
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())


# –∫–∞–∫–∏–µ —Ç–µ–≥–∏ —Ä–∞–∑—Ä–µ—à–∞–µ–º –æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ HTML (Telegram –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç subset)
ALLOWED_TAGS = {
    "a",
    "b",
    "i",
    "u",
    "s",
    "strong",
    "em",
    "code",
    "pre",
    "br",
    "tg-spoiler",
    "spoiler",
}

_tag_split_re = re.compile(r"(<[^>]*>)")
_tag_name_re = re.compile(r"^<\s*/?\s*([a-zA-Z0-9_-]+)")
_open_a_re = re.compile(r"<a\s+[^>]*>", re.IGNORECASE)
_close_a_re = re.compile(r"</a\s*>", re.IGNORECASE)


# ---------------------------------------------------------
#  URL UTILITIES ‚Äî –Ø–í–ù–û–ï –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–ï –°–°–´–õ–û–ö –î–õ–Ø GPT
# ---------------------------------------------------------

_url_re = re.compile(r"(https?://[^\s<]+)")


def extract_urls(text: str) -> list[str]:
    """–ò–∑–≤–ª–µ—á—å URL –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –Ω–µ –ø–æ–≤—Ä–µ–∂–¥–∞—è HTML."""
    return _url_re.findall(text)


def inject_markdown_links(text: str) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç URL -> [URL](URL)
    –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –ª—é–±–∞—è LLM, –≤–∫–ª—é—á–∞—è GPT-5, —Ç—Ä–∞–∫—Ç—É–µ—Ç URL –∫–∞–∫ —Å—Å—ã–ª–∫—É.
    """
    return _url_re.sub(r"[\1](\1)", text)


def strip_all_tags(text: str) -> str:
    # –ó–∞–º–µ–Ω—è–µ–º <...> –Ω–∞ —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–Ω—É—Ç—Ä–∏ href –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞
    # 1. –µ—Å–ª–∏ –µ—Å—Ç—å href="..." ‚Üí –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º—É —Å—Å—ã–ª–∫—É
    text = re.sub(r'<a[^>]*href=["\']([^"\']+)["\'][^>]*>', r"\1 ", text)

    # 2. —É–±–∏—Ä–∞–µ–º –≤–æ–æ–±—â–µ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏ —Ü–µ–ª–∏–∫–æ–º (<b>, </a>, <i> –∏ –ª—é–±—ã–µ)
    text = re.sub(r"<[^>]+>", "", text)

    # 3. –ß–∏—Å—Ç–∏–º html-—Å—É—â–Ω–æ—Å—Ç–∏ (&lt; ‚Üí < –∏ —Ç.–ø.)
    text = unescape(text)

    # 4. –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r" *\n\s*", "\n", text)

    return text.strip()


def split_into_chunks_by_words(text: str, max_len: int):
    if len(text) <= max_len:
        return [text]
    words = text.split(" ")
    chunks = []
    cur = ""
    for w in words:
        if cur and len(cur) + 1 + len(w) > max_len:
            chunks.append(cur)
            cur = w
        else:
            cur = cur + (" " if cur else "") + w
    if cur:
        chunks.append(cur)
    return chunks


path = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
config = load_config(os.path.join(BASE_DIR, ".env"))

# OpenAI –∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
openai_client = AsyncOpenAI(
    base_url=os.getenv("OPENAI_API_BASE"),
    api_key=config.open_ai.token,
)


def semantic_search(query: str, top_k: int = 5, segment: str | None = None):
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        api_key=os.getenv("OPENAI_API_EMBEDDING_KEY", ""),
        base_url=os.getenv("OPENAI_API_EMBEDDING_BASE"),
    )
    vectordb = FAISS.load_local(
        VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True
    )

    # –ë–µ—Ä–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ñ–∏–ª—å—Ç—Ä—É segment
    flt = {"segment": segment} if segment else None
    results = vectordb.similarity_search(query, k=top_k, filter=flt)

    return results


async def kb_query_impl(query: str):
    """–ü–æ–∏—Å–∫ –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    logger_bot.info(f"üîç –ò—â–µ–º –æ—Ç–≤–µ—Ç –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ. –ó–∞–ø—Ä–æ—Å: '{query}'")
    results = []

    # FAISS-–ø–æ–∏—Å–∫ (–ª–µ–∫—Ü–∏–∏)
    try:
        docs = semantic_search(query, top_k=3, segment="lectures")
        for doc in docs:
            results.append(
                {
                    "text": doc.page_content[:500],
                    "score": 0.8,
                    "source": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –∫–∞–Ω–∞–ª–∞ DOMOS Club",
                    "source_id": "faiss_lectures",
                    "is_specific": True,
                }
            )
    except Exception as e:
        logger_bot.error(f"‚ùå –û—à–∏–±–∫–∞ FAISS lectures: {e}")

    # FAISS-–ø–æ–∏—Å–∫ (–ñ–ö)
    try:
        docs = semantic_search(query, top_k=20, segment="real_estate")
        for doc in docs:
            results.append(
                {
                    "text": doc.page_content[:500],
                    "score": 0.8,
                    "source": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ñ–ö (—Å–∞–π—Ç—ã nmarket –∏ trendagent)",
                    "source_id": "faiss_real_estate",
                    "is_specific": True,
                }
            )
    except Exception as e:
        logger_bot.error(f"‚ùå –û—à–∏–±–∫–∞ FAISS real_estate: {e}")

    # # FAISS-–ø–æ–∏—Å–∫ (–î–æ–∫—É–º–µ–Ω—Ç—ã)
    # try:
    #     converted_docs_path = os.path.join(path, "converted_docs")
    #     if os.path.exists(converted_docs_path):
    #         for filename in os.listdir(converted_docs_path):
    #             if filename.endswith(".md"):
    #                 file_path = os.path.join(converted_docs_path, filename)
    #                 with open(file_path, "r", encoding="utf-8") as f:
    #                     content = f.read()
    #                     if query.lower() in content.lower():
    #                         results.append({
    #                             "text": content[:500],
    #                             "score": 0.7,
    #                             "source": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (–¥–æ–∫—É–º–µ–Ω—Ç—ã)",
    #                             "source_id": f"converted_docs_{filename}",
    #                             "is_specific": True
    #                         })
    # except Exception as e:
    #     logger_bot.error(f"‚ùå –û—à–∏–±–∫–∞ converted_docs: {e}")

    if not results:
        results.append(
            {
                "text": "–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏ Domosclub —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –æ –ñ–ö, —Ü–µ–Ω–∞—Ö –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö –∫–≤–∞—Ä—Ç–∏—Ä.",
                "score": 0.3,
                "source": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (–æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)",
                "source_id": "general_info",
                "is_specific": False,
            }
        )

    return json.dumps(results[:10], ensure_ascii=False)


user_histories: dict[int, list[str]] = {}


async def run_chat_with_tools(user_id: int, user_msg: str, max_history: int = 3):
    """
    –ü—Ä–æ—Å—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫:
    - –î–µ–ª–∞–µ—Ç web_search_preview —á–µ—Ä–µ–∑ GPT-5
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    """
    global user_histories

    def extract_text(resp):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –æ—Ç–≤–µ—Ç–∞ –ø–æ–¥ GPT-5 –∏ GPT-4.1"""
        if hasattr(resp, "output_text") and resp.output_text:
            return resp.output_text.strip()
        if hasattr(resp, "choices") and resp.choices:
            msg = getattr(resp.choices[0], "message", None)
            if msg and getattr(msg, "content", None):
                return msg.content.strip()
        return ""

    # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    history = user_histories.get(user_id, [])
    history.append(user_msg)
    if len(history) > max_history:
        history = history[-max_history:]  # –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N
    user_histories[user_id] = history  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ

    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
    vector_response = await kb_query_impl(user_msg)
    vector_data = json.loads(vector_response)
    sources = [item["source"] for item in vector_data]

    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º vector_data –≤ —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –ò–ò
    vector_texts = []
    for item in vector_data:
        vector_texts.append(item.get("text", ""))
    vector_text = "\n\n".join(vector_texts)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è GPT-5
    history_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(history)])
    combined_input = f"""
    –¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –≤ –∫–æ–º–ø–∞–Ω–∏–∏ Domosclub.
    –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞:
    ‚Ä¢ –†–∞–∑—Ä–µ—à–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¢–û–õ–¨–ö–û —Å —Å–∞–π—Ç–æ–≤:
        - https://avito.ru
        - https://cian.ru
        - https://domclick.ru

    –ï—Å–ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–∏—Å–∫–∞ –≤—ã–¥–∞—ë—Ç —Å—Å—ã–ª–∫–∏ —Å –¥—Ä—É–≥–∏—Ö —Å–∞–π—Ç–æ–≤ ‚Äì –∏–≥–Ω–æ—Ä–∏—Ä—É–π –∏—Ö –∏ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –≤ –æ—Ç–≤–µ—Ç–µ.

    –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
    1. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ—Ä–∞–∑—ã "—è –Ω–µ –∑–Ω–∞—é", "–æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç", "—è –ø—Ä–æ–≤–µ–¥—É –ø–æ–∏—Å–∫" –∏–ª–∏ "—Å–µ–π—á–∞—Å –Ω–∞–π–¥—É".
    2. –í—Å–µ–≥–¥–∞ –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:
        - –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –±–∞–∑–∞ ‚Äî –î–ª—è —É–∫–∞–∑–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π {' '.join(sources)}. –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–π 1 —Ä–∞–∑.
        - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ ‚Äî —Å—Å—ã–ª–∫–∏ –≤ HTML-—Ñ–æ—Ä–º–∞—Ç–µ.
        - –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã ‚Äî –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ HTML.

    –ò—Å–ø–æ–ª—å–∑—É–π –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞: {history_text}
    –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –±–∞–∑—ã: {vector_text}

    ========================================================
    –ü–†–ê–í–ò–õ–ê –û–ë–†–ê–ë–û–¢–ö–ò –°–°–´–õ–û–ö:

    1. –õ—é–±–∞—è —Å—Ç—Ä–æ–∫–∞, –Ω–∞—á–∏–Ω–∞—é—â–∞—è—Å—è —Å http:// –∏–ª–∏ https://, –≤—Å–µ–≥–¥–∞ —è–≤–ª—è–µ—Ç—Å—è —Å—Å—ã–ª–∫–æ–π.
    2. –ï—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è URL, –æ–Ω –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–µ–Ω –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–∞.
    3. URL, –Ω–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—ã–µ —Ç–µ–≥–æ–º <a>, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç—Ä–∞–∫—Ç—É–π –∫–∞–∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏.
    4. –ï—Å–ª–∏ URL –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ Markdown-—Ñ–æ—Ä–º–∞—Ç–µ [URL](URL) ‚Äî —Å—á–∏—Ç–∞–π –µ–≥–æ –∫–ª–∏–∫–∞–±–µ–ª—å–Ω–æ–π –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–æ–π.
    5. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∏–∑–º–µ–Ω—è–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É URL –∏ –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞–π —Å—Å—ã–ª–∫–∏ –Ω–∞ —á–∞—Å—Ç–∏.
    6. –í –æ—Ç–≤–µ—Ç–µ –≤—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–π URL –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –≤–∏–¥–µ –∏–ª–∏ –≤ HTML (<a href="URL">URL</a>).
    ========================================================

    –û—Ç–≤–µ—Ç—å –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_msg}
    """

    try:
        # –í—ã–ø–æ–ª–Ω—è–µ–º web_search_preview
        web_response = await openai_client.responses.create(
            model="openai/gpt-5",
            tools=[
                {
                    "type": "web_search_preview",
                    "user_location": {
                        "type": "approximate",
                        "country": "RU",
                        "city": "Ekaterinburg",
                        "region": "Ekaterinburg",
                    },
                    "search_context_size": "low",
                }
            ],
            input=combined_input,
        )

        text = extract_text(web_response)
        final_answer = text or "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç."
        return final_answer

    except Exception as e:
        logger_bot.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ run_chat_with_tools: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞."


async def handle_gpt_message(message: types.Message):
    user_id = message.from_user.id
    banned = getBannedUserId(user_id)

    if banned != 0:
        await message.answer(
            "‚≠ï  –í–∞—à –∞–∫–∫–∞—É–Ω—Ç –∑–∞–±–∞–Ω–µ–Ω, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –±–æ—Ç–∞!"
        )
        return

    payed = getUserPay(user_id)
    if payed != 1:
        await message.answer("‚≠ï –°–Ω–∞—á–∞–ª–∞ –æ–ø–ª–∞—Ç–∏—Ç–µ –ø–æ–¥–ø–∏—Å–∫—É!")
        return

    if message.text.startswith("/"):
        return

    logger_bot.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
    logger_bot.info(f"   –¢–µ–∫—Å—Ç: '{message.text}'")

    original_user_msg = message.text
    user_msg_normalized = inject_markdown_links(original_user_msg)

    wait_msg = await message.reply("‚è≥ –ü–æ–¥–æ–∂–¥–∏—Ç–µ –Ω–µ–º–Ω–æ–≥–æ, –∑–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è...")
    logger_bot.info("   üí¨ –£–≤–µ–¥–æ–º–∏–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ —Å—Ç–∞—Ä—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏...")

    answer = None  # üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û

    try:
        logger_bot.info("   üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏...")
        answer = await run_chat_with_tools(
            user_id=user_id, user_msg=user_msg_normalized
        )

        if not answer or not answer.strip():
            raise ValueError("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç GPT")

        chunks = split_into_chunks_by_words(answer, MAX_BOT_MSG_LENGTH)
        await wait_msg.edit_text(chunks[0], parse_mode="HTML")
        for c in chunks[1:]:
            await message.answer(c, parse_mode="HTML")

    except Exception as e:
        logger_bot.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–≤–µ—Ç–∞: {e}")

        # –µ—Å–ª–∏ answer —É–∂–µ –µ—Å—Ç—å ‚Äî –ø—Ä–æ–±—É–µ–º –æ—á–∏—Å—Ç–∏—Ç—å HTML
        if answer:
            try:
                logger_bot.info("–ü—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å HTML-—Ç–µ–≥–∏")
                safe_answer = strip_all_tags(answer)
                new_chunks = split_into_chunks_by_words(
                    safe_answer, MAX_BOT_MSG_LENGTH
                )
                await wait_msg.edit_text(new_chunks[0])
                for c in new_chunks[1:]:
                    await message.answer(c)
                return
            except Exception as e2:
                logger_bot.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ HTML: {e2}")

        await wait_msg.edit_text(
            "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞."
        )

def register_yandex_gpt(dp):
    dp.register_message_handler(
        handle_gpt_message,
        lambda message: message.chat.type == "private",
        content_types=types.ContentTypes.TEXT,
        state=None,
    )
