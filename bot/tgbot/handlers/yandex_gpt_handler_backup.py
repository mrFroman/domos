import json
import os
import re
from html import escape

from aiogram import types
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from openai import AsyncOpenAI

from config import BASE_DIR, MAX_BOT_MSG_LENGTH, VECTOR_DB_PATH, load_config, logger_bot


# –∫–∞–∫–∏–µ —Ç–µ–≥–∏ —Ä–∞–∑—Ä–µ—à–∞–µ–º –æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ HTML (Telegram –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç subset)
ALLOWED_TAGS = {
    "a", "b", "i", "u", "s", "strong", "em", "code", "pre", "br", "tg-spoiler", "spoiler"
}

_tag_split_re = re.compile(r'(<[^>]*>)')
_tag_name_re = re.compile(r'^<\s*/?\s*([a-zA-Z0-9_-]+)')
_open_a_re = re.compile(r'<a\s+[^>]*>', re.IGNORECASE)
_close_a_re = re.compile(r'</a\s*>', re.IGNORECASE)


def sanitize_and_fix_html(text: str) -> str:
    # 0) –£–±–∏—Ä–∞–µ–º "–ø–æ–ª—É—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ" <a ...>, –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –∏—Ö –≤ –ø–æ–ª–Ω–æ—Å—Ç—å—é –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ç–µ–∫—Å—Ç
    text = re.sub(r'&lt;a\s+([^>]*)>', r'&lt;a \1&gt;', text, flags=re.IGNORECASE)

    # 1) –Ω–µ–π—Ç—Ä–∞–ª–∏–∑—É–µ–º –≤—Å–µ "–Ω–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ" —Ç–µ–≥–∏
    parts = _tag_split_re.split(text)
    out_parts = []
    for part in parts:
        if not part:
            continue
        if part.startswith('<') and part.endswith('>'):
            m = _tag_name_re.match(part)
            tag_name = m.group(1).lower() if m else None
            if tag_name and tag_name in ALLOWED_TAGS:
                out_parts.append(part)
            else:
                out_parts.append(escape(part))
        else:
            out_parts.append(part)
    safe = ''.join(out_parts)

    # 2) –¥–æ–ø–æ–ª–Ω—è–µ–º </a> –¥–ª—è –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã—Ö —Å—Å—ã–ª–æ–∫
    opens = [(m.start(), m.end()) for m in _open_a_re.finditer(safe)]
    closes = [m.start() for m in _close_a_re.finditer(safe)]
    used_close = [False] * len(closes)
    insert_positions = []

    for o_start, o_end in opens:
        paired = False
        for idx, cpos in enumerate(closes):
            if not used_close[idx] and cpos > o_end:
                used_close[idx] = True
                paired = True
                break
        if not paired:
            next_tag_idx = safe.find('<', o_end)
            insert_at = next_tag_idx if next_tag_idx != -1 else len(safe)
            insert_positions.append(insert_at)

    res = safe
    for pos in sorted(insert_positions, reverse=True):
        res = res[:pos] + '</a>' + res[pos:]

    return res


def split_into_chunks_by_words(text: str, max_len: int):
    if len(text) <= max_len:
        return [text]
    words = text.split(' ')
    chunks = []
    cur = ""
    for w in words:
        if cur and len(cur) + 1 + len(w) > max_len:
            chunks.append(cur)
            cur = w
        else:
            cur = cur + (' ' if cur else '') + w
    if cur:
        chunks.append(cur)
    return chunks


path = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
config = load_config(os.path.join(BASE_DIR, ".env"))

# OpenAI –∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
open_ai_token = config.open_ai.token
openai_client = AsyncOpenAI(api_key=open_ai_token)


def semantic_search(query: str, top_k: int = 5, segment: str | None = None):
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectordb = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)

    # –ë–µ—Ä–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ñ–∏–ª—å—Ç—Ä—É segment
    flt = {"segment": segment} if segment else None
    results = vectordb.similarity_search(query, k=top_k, filter=flt)
    
    return results


async def kb_query_impl(query: str):
    """–ü–æ–∏—Å–∫ –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    logger_bot.info(f"üîç –ò—â–µ–º –æ—Ç–≤–µ—Ç –≤ –ë–î –î–æ–º–æ—Å. –ó–∞–ø—Ä–æ—Å: '{query}'")
    results = []

    # FAISS-–ø–æ–∏—Å–∫ (–ª–µ–∫—Ü–∏–∏)
    try:
        docs = semantic_search(query, top_k=3, segment="lectures")
        for doc in docs:
            results.append({
                "text": doc.page_content[:500],
                "score": 0.8,
                "source": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (–ª–µ–∫—Ü–∏–∏)",
                "source_id": "faiss_lectures",
                "is_specific": True
            })
    except Exception as e:
        logger_bot.error(f"‚ùå –û—à–∏–±–∫–∞ FAISS lectures: {e}")

    # FAISS-–ø–æ–∏—Å–∫ (–ñ–ö)
    try:
        docs = semantic_search(query, top_k=20, segment="real_estate")
        for doc in docs:
            results.append({
                "text": doc.page_content[:500],
                "score": 0.8,
                "source": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ –ñ–ö",
                "source_id": "faiss_real_estate",
                "is_specific": True
            })
    except Exception as e:
        logger_bot.error(f"‚ùå –û—à–∏–±–∫–∞ FAISS real_estate: {e}")

    # FAISS-–ø–æ–∏—Å–∫ (–î–æ–∫—É–º–µ–Ω—Ç—ã)
    try:
        converted_docs_path = os.path.join(path, "converted_docs")
        if os.path.exists(converted_docs_path):
            for filename in os.listdir(converted_docs_path):
                if filename.endswith(".md"):
                    file_path = os.path.join(converted_docs_path, filename)
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                        if query.lower() in content.lower():
                            results.append({
                                "text": content[:500],
                                "score": 0.7,
                                "source": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (–¥–æ–∫—É–º–µ–Ω—Ç—ã)",
                                "source_id": f"converted_docs_{filename}",
                                "is_specific": True
                            })
    except Exception as e:
        logger_bot.error(f"‚ùå –û—à–∏–±–∫–∞ converted_docs: {e}")

    if not results:
        results.append({
            "text": "–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏ Domosclub —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –æ –ñ–ö, —Ü–µ–Ω–∞—Ö –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö –∫–≤–∞—Ä—Ç–∏—Ä.",
            "score": 0.3,
            "source": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (–æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)",
            "source_id": "general_info",
            "is_specific": False
        })

    return json.dumps(results[:10], ensure_ascii=False)


user_histories: dict[int, list[str]] = {}

async def run_chat_with_tools(user_id: int, user_msg: str, max_history: int = 3):
    """
    –ü—Ä–æ—Å—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫:
    - –î–µ–ª–∞–µ—Ç web_search_preview —á–µ—Ä–µ–∑ GPT-5
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    """
    global user_histories

    def extract_text(resp):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –æ—Ç–≤–µ—Ç–∞ –ø–æ–¥ GPT-5 –∏ GPT-4.1"""
        if hasattr(resp, "output_text") and resp.output_text:
            return resp.output_text.strip()
        if hasattr(resp, "choices") and resp.choices:
            msg = getattr(resp.choices[0], "message", None)
            if msg and getattr(msg, "content", None):
                return msg.content.strip()
        return ""

    # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    history = user_histories.get(user_id, [])
    history.append(user_msg)
    if len(history) > max_history:
        history = history[-max_history:]  # –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N
    user_histories[user_id] = history  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ

    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
    vector_response = await kb_query_impl(user_msg)
    vector_data = json.loads(vector_response)
    
    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º vector_data –≤ —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –ò–ò
    vector_texts = []
    for item in vector_data:
        vector_texts.append(item.get("text", ""))
    vector_text = "\n\n".join(vector_texts)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è GPT-5
    history_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(history)])
    combined_input = f"""
    –¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –≤ –∫–æ–º–ø–∞–Ω–∏–∏ Domosclub.

    –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞:

    1. –í–°–ï —Å—Å—ã–ª–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ HTML-—Ñ–æ—Ä–º–∞—Ç–µ:
    <a href="URL">–ù–∞–∑–≤–∞–Ω–∏–µ —Å–∞–π—Ç–∞ –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ</a>
    2. –ù–ò–ö–ê–ö–ò–• –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Å—Å—ã–ª–æ–∫:
    - –ù–∏–∫–∞–∫–∏—Ö Markdown ([—Ç–µ–∫—Å—Ç](URL))
    - –ù–∏–∫–∞–∫–∏—Ö –∫—Ä—É–≥–ª—ã—Ö –∏–ª–∏ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–æ–∫ –≤–æ–∫—Ä—É–≥ —Å—Å—ã–ª–æ–∫
    - –ù–∏–∫–∞–∫–∏—Ö raw URLs –±–µ–∑ <a>
    3. –ó–∞–∫—Ä—ã–≤–∞–π –≤—Å–µ —Ç–µ–≥–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. –ö–∞–∂–¥—ã–π <a> –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å </a>.
    4. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ—Ä–∞–∑—ã "—è –Ω–µ –∑–Ω–∞—é", "–æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç", "—è –ø—Ä–æ–≤–µ–¥—É –ø–æ–∏—Å–∫" –∏–ª–∏ "—Å–µ–π—á–∞—Å –Ω–∞–π–¥—É".
    5. –í—Å–µ–≥–¥–∞ –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:
    - –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –±–∞–∑–∞ ‚Äî "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –î–æ–º–æ—Å" + —Å—Å—ã–ª–∫–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
    - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ ‚Äî —Å—Å—ã–ª–∫–∏ –≤ HTML-—Ñ–æ—Ä–º–∞—Ç–µ
    - –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã ‚Äî –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ HTML
    6. –ù–µ –≤—Å—Ç–∞–≤–ª—è–π –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –≤–æ–∫—Ä—É–≥ —Å—Å—ã–ª–æ–∫ (–Ω–µ—Ç —Å–∫–æ–±–æ–∫, –∫–∞–≤—ã—á–µ–∫, —Ç–æ—á–µ–∫ –ø–µ—Ä–µ–¥/–ø–æ—Å–ª–µ, –∫—Ä–æ–º–µ —Ç–µ—Ö, —á—Ç–æ –≤ HTML-—Ç–µ–≥–µ).

    –ò—Å–ø–æ–ª—å–∑—É–π –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞: {history_text}
    –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –±–∞–∑—ã: {vector_text}

    –û—Ç–≤–µ—Ç—å –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_msg}
    """

    try:
        # –í—ã–ø–æ–ª–Ω—è–µ–º web_search_preview
        web_response = await openai_client.responses.create(
            model="gpt-5",
            tools=[{
                "type": "web_search_preview",
                "user_location": {
                    "type": "approximate",
                    "country": "RU",
                    "city": "Ekaterinburg",
                    "region": "Ekaterinburg",
                },
                "search_context_size": "low",
            }],
            input=combined_input,
        )

        text = extract_text(web_response)
        final_answer = text or "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç."
        return final_answer

    except Exception as e:
        logger_bot.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ run_chat_with_tools: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞."


async def handle_gpt_message(message: types.Message):
    user_id = message.from_user.id
    
    if message.text.startswith('/'):
        return
    logger_bot.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {message.from_user.id}")
    logger_bot.info(f"   –¢–µ–∫—Å—Ç: '{message.text}'")
        
    wait_msg = await message.reply("‚è≥ –ü–æ–¥–æ–∂–¥–∏—Ç–µ –Ω–µ–º–Ω–æ–≥–æ, –∑–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è...")
    logger_bot.info("   üí¨ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é...")
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
        answer = await run_chat_with_tools(user_id=user_id, user_msg=message.text)
        print(f'{answer=}')
        safe_answer = sanitize_and_fix_html(answer)
        print(f'{safe_answer=}')
        logger_bot.info("   üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏...")

        if safe_answer and safe_answer.strip():
            logger_bot.info(f"   ‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç ({len(safe_answer)} —Å–∏–º–≤–æ–ª–æ–≤), –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é")

        chunks = split_into_chunks_by_words(safe_answer, MAX_BOT_MSG_LENGTH)
        await wait_msg.edit_text(chunks[0], parse_mode="HTML")
        for c in chunks[1:]:
            await message.answer(c, parse_mode="HTML")

    except Exception as e:
        logger_bot.error(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ handle_gpt_message: {e}")
        try:
            # TODO –î–æ–¥–µ–ª–∞—Ç—å —Ç—É—Ç
            ...
        except:
            await wait_msg.edit_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞.")


def register_yandex_gpt(dp):
    dp.register_message_handler(
        handle_gpt_message,
        lambda message: message.chat.type == "private",
        content_types=types.ContentTypes.TEXT,
        state=None
    )
